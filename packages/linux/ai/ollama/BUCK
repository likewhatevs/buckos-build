load("//defs:package.bzl", "package")

# =============================================================================
# Ollama - Run large language models locally
# =============================================================================
#
# Ollama is a tool for running large language models (LLMs) locally.
# It provides:
# - Easy model management (pull, run, create)
# - REST API for model inference
# - Support for popular models (Llama, Mistral, Code Llama, etc.)
# - GPU acceleration with CUDA and ROCm
#
# Usage:
#   ollama pull llama3.2       # Download a model
#   ollama run llama3.2        # Run interactive chat
#   ollama serve               # Start API server
#
# =============================================================================

package(
    build_rule = "binary",
    name = "ollama",
    version = "0.4.6",
    url = "https://github.com/ollama/ollama/archive/refs/tags/v0.4.6.tar.gz",
    sha256 = "2cd8a6140d5a0c5bbf8143310743db58cf1083fe95b8bc45651244cd5c3b9984",
    description = "Get up and running with large language models locally",
    homepage = "https://ollama.ai/",
    license = "MIT",
    install_script = """
# Install ollama binary from $SRCS (extracted tarball)
mkdir -p "$OUT/usr/bin"
install -m 755 "$SRCS/bin/ollama" "$OUT/usr/bin/ollama"

# Install CUDA/ROCm libraries
mkdir -p "$OUT/usr/lib/ollama"
cp -r "$SRCS/lib/ollama/"* "$OUT/usr/lib/ollama/" 2>/dev/null || true

# Create ollama user home directory for models
mkdir -p "$OUT/var/lib/ollama"

# Create systemd service file
mkdir -p "$OUT/etc/systemd/system"
cat > "$OUT/etc/systemd/system/ollama.service" << 'EOF'
[Unit]
Description=Ollama Service
After=network-online.target

[Service]
ExecStart=/usr/bin/ollama serve
User=ollama
Group=ollama
Restart=always
RestartSec=3
Environment="HOME=/var/lib/ollama"
Environment="OLLAMA_HOST=0.0.0.0"

[Install]
WantedBy=default.target
EOF

# Create default environment configuration
mkdir -p "$OUT/etc/ollama"
cat > "$OUT/etc/ollama/ollama.conf" << 'EOF'
# Ollama configuration
# OLLAMA_HOST=0.0.0.0:11434
# OLLAMA_MODELS=/var/lib/ollama/models
# OLLAMA_NUM_PARALLEL=1
# OLLAMA_MAX_LOADED_MODELS=1
EOF
""",
    visibility = ["PUBLIC"],
)
