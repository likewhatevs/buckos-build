load("@root//defs:package_defs.bzl", "python_package")

python_package(
    name = "flash-attention",
    version = "2.7.3",
    src_uri = "https://github.com/Dao-AILab/flash-attention/archive/refs/tags/v2.7.3.tar.gz",
    sha256 = "21a7b82f787d2a33905c45ba10c3275d504c408b744520f7691d9501b7b4c009",
    deps = [
        "ai//ml-frameworks/pytorch:pytorch",
        "dev-libs//cuda/cutlass:cutlass",
        "dev-libs//python/einops:einops",
        "dev-libs//python/packaging:packaging",
        "dev-libs//python/ninja:ninja",
        "dev-libs//python/psutil:psutil",
    ],
)
